{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U torchtext\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入Multi30k数据集并做基本处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# 定义token的字典, 定义vocab字典\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "# 创建源语言和目标语言的Tokenizer, 确保依赖关系已经安装\n",
    "#!pip install -U spacy\n",
    "!python -m spacy download en\n",
    "!python -m spacy download de\n",
    "import de_core_news_sm\n",
    "import en_core_web_sm\n",
    "\n",
    "# get_tokenizer是分词函数, 如果没有特殊的则按照英语的空格分割, 如果有这按照对应的分词库返回. 比如spacy, 返回对应的分词库\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建生成分词的迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "    # data_iter: 对象的迭代对象 Multi30k对象\n",
    "    # language: 对应的翻译语言 {'de': 0, 'en': 1}\n",
    "    \n",
    "    for data_sample in data_iter:\n",
    "        # data_sample:(德文, 英文)\n",
    "        # data_sample:('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\\n', 'Two young, White males are outside near many bushes.\\n')\n",
    "        # token_transform['de']()=['Zwei', 'junge', 'weiße', 'Männer', 'sind', 'im', 'Freien', 'in', 'der', 'Nähe', 'vieler', 'Büsche', '.', '\\n']\n",
    "        # or  token_transform['en']分别进行构造对应的字典\n",
    "\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# 定义特殊字符并下载数据设置默认索引\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "# 确保标记按其索引的顺序正确插入到词汇表中\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # 训练数据集的迭代器,\n",
    "    # 数据集是用英文描述图像的英文语句, 然后人工将其翻译为德文的语句,有两个文件, 一个是train.zh 一个是train.en文件,\n",
    "    # 然后将其构建为(德文, 英文)的形式\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # 创建torchtext的vocab对象, 即词汇表\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# 将 UNK_IDX 设置为默认索引。未找到令牌时返回此索引\n",
    "# 如果未设置，则在 Vocabulary 中找不到查询的标记时抛出 RuntimeError\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入网络搭建所需库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义位置编码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,emb_size: int,dropout: float, maxlen: int = 5000):\n",
    "        \n",
    "        # emb_size: 词嵌入的维度大小\n",
    "        # dropout: 正则化的大小\n",
    "        # maxlen: 句子的最大长度\n",
    "        \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 将1000的2i/d_model变型为e的指数形式\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        # 效果等价与torch.arange(0, maxlen).unsqueeze(1)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        # 构建一个(maxlen, emb_size)大小的全零矩阵\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        # 偶数列是正弦函数填充\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        # 奇数列是余弦函数填充\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        # 将其维度变成三维, 为了后期方便计算\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        # 添加dropout层, 防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 向模块添加持久缓冲区。\n",
    "        # 这通常用于注册不应被视为模型参数的缓冲区。例如，pos_embedding不是一个参数，而是持久状态的一部分。\n",
    "        # 缓冲区可以使用给定的名称作为属性访问。\n",
    "        # 说明：\n",
    "        # 应该就是在内存中定义一个常量，同时，模型保存和加载的时候可以写入和读出\n",
    "        \n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        # 将token_embedding和位置编码相融合\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义词嵌入层类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        \n",
    "        # vocab_size:词表的大小\n",
    "        # emb_size:词嵌入的维度\n",
    "        \n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        # 调用nn中的预定义层Embedding, 获取一个词嵌入对象self.embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        # 将emb_size传入类内, 变成类内的变量\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        # 让 embeddings vector 在增加 之后的 postion encoing 之前相对大一些的操作，\n",
    "        # 主要是为了让position encoding 相对的小，这样会让原来的 embedding vector 中的信息在和 position encoding 的信息相加时不至于丢失掉\n",
    "        # 让 embeddings vector 相对大一些\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建Seq2SeqTransformer模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,num_encoder_layers, num_decoder_layers,emb_size,nhead,src_vocab_size,tgt_vocab_size,dim_feedforward = 512,dropout = 0.1):\n",
    "        \n",
    "        # num_encoder_layers: 编码器的层数\n",
    "        # num_decoder_layers: 解码器的层数\n",
    "        # emb_size: 词嵌入的维度\n",
    "        # nhead: 头数\n",
    "        # src_vocab_size: 源语言的词表大小\n",
    "        # tgt_vocab_size: 目标语言的词表大小\n",
    "        # dim_feedforward: 前馈全连接层的维度\n",
    "        # dropout: 正则化的大小\n",
    "        \n",
    "        # 继承nn.Module类, 一般继承习惯行的写法\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        # 创建Transformer对象\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        # 创建全连接线性层\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        # 创建源语言的embedding层\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        # 创建目标语言的embedding层\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        # 创建位置编码器层对象\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        \n",
    "        # src: 源语言\n",
    "        # trg: 目标语言\n",
    "        # src_mask: 源语言掩码\n",
    "        # tgt_mask: 目标语言掩码\n",
    "        # src_padding_mask: 源语言的padding_mask\n",
    "        # tgt_padding_mask: 目标语言的padding_mask\n",
    "        # memory_key_padding_mask: 中间语义张量的padding_mask\n",
    "        \n",
    "        # 获取源语言的embedding张量融合了位置编码\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        # 获取目标语言的embedding张量融合了位置编码\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        # 经过Transformer进行编解码之后输出out值\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        # outs值经过输出层得到最后的输出分布值\n",
    "        return self.generator(outs)\n",
    "    # 定义Transformer的编码器\n",
    "    def encode(self, src, src_mask):\n",
    "        \n",
    "        # src:源语言\n",
    "        # src_mask:源语言掩码\n",
    "        \n",
    "        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n",
    "    # 定义Transformer的解码器\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        \n",
    "        # tgt:目标语言\n",
    "        # memory:中间语言张量输出\n",
    "        # tgt_mask: 目标语言的掩码\n",
    "        \n",
    "        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义掩码\n",
    "\n",
    "作用是防止模型在进行预测的过程中查看到未来的单词. 同时需要掩码来隐藏源语言和目标语言的padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    # sz: 句子的长度\n",
    "    # triu生成的是上三角, 经过transpose之后变成了下三角矩阵\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    # 将0的位置填充负无穷小, 将1的位置填充为0\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    \n",
    "    # src: 源语言张量形状为: [seq_length , batch_size]\n",
    "    # tgt: 目标语言张量形状为: [seq_length , batch_size]\n",
    "    \n",
    "    # 获取源语言的句子长度\n",
    "    src_seq_len = src.shape[0]\n",
    "    # 获取目标语言的句子长度\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "    # 产生目标语言的掩码张量\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    # 产生源语言的掩码张量\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "    # 构建源语言的padding_mask  src_padding_mask==> [batch_size, seq_len]\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    # 构建目标语言的padding_mask tgt_paddig_mask ==> [batch_size, seq_len-1]\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 1024\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "model_path = './transformer_translation.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实例化模型并定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置种子用于生成随机数，以使得结果是确定的\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# 设置调用时候使用的参数\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512 #d_model 1_512\n",
    "NHEAD = 8 #h 1_8\n",
    "FFN_HID_DIM = 2048 #d_ff 1_2048\n",
    "BATCH_SIZE = 128 \n",
    "NUM_ENCODER_LAYERS = 6 #N 1_6\n",
    "NUM_DECODER_LAYERS = 6 #N 1_6\n",
    "\n",
    "\n",
    "# 实例化Transformer对象\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "\n",
    "\n",
    "# 为了保证每层的输入和输出的方差相同, 防止梯度消失问题\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        # 此处使用的是xavier的均匀分布\n",
    "        nn.init.xavier_uniform_(p)\n",
    "        \n",
    "# 如果有GPU则将模型移动到GPU\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "# 定义损失函数\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "# 定义优化器  betas: 用于计算梯度及其平方的运行平均值的系数  eps:添加到分母以提高数值稳定性\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9) # 1_0.0001\n",
    "#自适应调整学习率\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=2, min_lr=0, eps=1e-08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将字符串转化为整数的tensor张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在句子首尾添加起始和结束符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数, 完成句子首尾BOS/EOS的添加过程\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    # 添加的是列表形式的数据, 将BOS和EOS添加到句子的首尾部分\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "text_transform = {}\n",
    "# 循环添加源语言和目标语言\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据进行批次化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照批次进行源语言和目标语言的组装\n",
    "def collate_fn(batch):\n",
    "    # 定义源语言和目标语言的批次列表\n",
    "    src_batch, tgt_batch = [], []\n",
    "    # 循环批次样本\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        # 添加源语言句子到列表中\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        # 添加目标语言句子到列表中\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "    # 将源语言和目标语言进行截断补齐  PAD_IDX=1\n",
    "    # src_batch的形状为: [seq_length, batch]  seq_length是最长的句子长度\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    # tgt_batch的形状为: [seq_length, batch]  seq_length是最长的句子长度\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义批次训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    # 开启训练模式\n",
    "    model.train()\n",
    "    # 定义其实的损失值为0\n",
    "    losses = 0\n",
    "    # 获取训练数据集的迭代器, 语言对为(zh, en)\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # 加载数据, 按照一个批次一个批次进行加载, 返回一个迭代器\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    # 循环数据迭代器\n",
    "    for src, tgt in train_dataloader:\n",
    "        # 将源语言数据移动到对应的设备上去\n",
    "        src = src.to(DEVICE)\n",
    "        # 将目标语言数据移动到对应设备上去\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        # 获取输入真实的张量 第一个单词到倒数第二个单词\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        # 调用mask函数, 生成对应的四个mask\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        # 调用模型进行训练, 得到最后的张量分布\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 获取输出真实的标签数据  第二个单词到最后一个单词\n",
    "        tgt_out = tgt[1:, :]\n",
    "        # 计算损失\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 梯度更新\n",
    "        optimizer.step()\n",
    "\n",
    "        # 损失值累加求和\n",
    "        losses += loss.item()\n",
    "    # 返回平均损失值\n",
    "    return losses / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义批次评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    # 开启模型评估模式\n",
    "    model.eval()\n",
    "    # 定义起始损失值\n",
    "    losses = 0\n",
    "    # 加载验证数据集, 语言对为(zh, en)\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # 返回验证集的数据加载器\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "    # 循环验证集\n",
    "    for src, tgt in val_dataloader:\n",
    "        # 源语言数据移动到对应的设备上\n",
    "        src = src.to(DEVICE)\n",
    "        # 目标语言数据移动到对应的设备上\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        # 获取输入的真实的张量\n",
    "        tgt_input = tgt[:-1, :]\n",
    "        # 调用mask函数, 产生对应的四个mask值\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        # 调用模型, 得到对应的输出分布值\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        # 获取输出的真实张量\n",
    "        tgt_out = tgt[1:, :]\n",
    "        # 计算损失值\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        # 损失值累加, 求和\n",
    "        losses += loss.item()\n",
    "    # 求得对应的平均损失\n",
    "    return losses / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "TLOSS = []\n",
    "VLOSS = []\n",
    "TN = []\n",
    "min_loss = 100000 # 随便设置一个比较大的数\n",
    "\n",
    "# 循环整个数据集num_epochs次\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    # 获取开始时间\n",
    "    start_time = timer()\n",
    "    # 将整个训练数据集进行训练\n",
    "    train_loss = train_epoch(transformer, optimizer\n",
    "    # 获取结束时间\n",
    "    end_time = timer()\n",
    "    # 将整个验证集进行评估\n",
    "    val_loss = evaluate(transformer)\n",
    "    tn = end_time - start_time\n",
    "    TLOSS.append(train_loss)\n",
    "    VLOSS.append(val_loss)\n",
    "    TN.append(tn)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # 打印每个epoch的训练损失, 验证损失, 和训练时间.\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {tn:.3f}s\"))\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        print(\"save model\")\n",
    "        torch.save(transformer.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 展示结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "x=range(0,NUM_EPOCHS)\n",
    "plt.plot(x,TLOSS,label='train_loss',color='green') #设置折线颜色和标签\n",
    "plt.plot(x,VLOSS,label='val_loss',color='blue')\n",
    "plt.xticks(x[::10])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "#绘制网格\n",
    "plt.grid(alpha=0.4,linestyle='-.')  #alpha设置网格透明度\n",
    "#添加图例\n",
    "plt.legend(loc='upper right') #设置图例字体及其位置，可以不写loc参数，默认为右上\n",
    "plt.savefig(\"./wmt18_10.jpg\")\n",
    "plt.show()\n",
    "\n",
    "print(min_loss)\n",
    "print(sum(TN)/len(TN))\n",
    "transformer.load_state_dict(torch.load(model_path))\n",
    "print('加载成功')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 翻译函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用贪心算法构建生成序列函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义最终的翻译转化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
